{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10296633,"sourceType":"datasetVersion","datasetId":6373014},{"sourceId":10297179,"sourceType":"datasetVersion","datasetId":6373428}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as nnf\nfrom torch.utils.data import Dataset, DataLoader\nfrom enum import Enum\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport os\nimport pickle\nimport sys\nimport argparse\nimport json\nfrom typing import Tuple, Optional, Union\nfrom torch.utils.tensorboard import SummaryWriter\n\n%load_ext tensorboard","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:18:49.301183Z","iopub.execute_input":"2024-12-26T07:18:49.301540Z","iopub.status.idle":"2024-12-26T07:19:02.091839Z","shell.execute_reply.started":"2024-12-26T07:18:49.301491Z","shell.execute_reply":"2024-12-26T07:19:02.091090Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class MappingType(Enum):\n    MLP = 'mlp'\n    Transformer = 'transformer'\n\n\nclass ClipCocoDataset(Dataset):\n\n    def __len__(self) -> int:\n        return len(self.captions_tokens)\n\n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)  # mask is zero where we out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        return tokens, mask\n\n    def __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[self.caption2embedding[item]]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        return tokens, mask, prefix\n\n    def __init__(self, data_path: str,  prefix_length: int, gpt2_type: str = \"gpt2\",\n                 normalize_prefix=False):\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        with open(data_path, 'rb') as f:\n            all_data = pickle.load(f)\n        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n        sys.stdout.flush()\n        self.prefixes = all_data[\"clip_embedding\"]\n        captions_raw = all_data[\"captions\"]\n        self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n        self.captions = [caption['caption'] for caption in captions_raw]\n        if os.path.isfile(f\"{data_path[:-4]}_tokens.pkl\"):\n            with open(f\"{data_path[:-4]}_tokens.pkl\", 'rb') as f:\n                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)\n        else:\n            self.captions_tokens = []\n            self.caption2embedding = []\n            max_seq_len = 0\n            for caption in captions_raw:\n                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption['caption']), dtype=torch.int64))\n                self.caption2embedding.append(caption[\"clip_embedding\"])\n                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n            # self.max_seq_len = max_seq_len\n            with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:19:02.093163Z","iopub.execute_input":"2024-12-26T07:19:02.093799Z","iopub.status.idle":"2024-12-26T07:19:02.105583Z","shell.execute_reply.started":"2024-12-26T07:19:02.093764Z","shell.execute_reply":"2024-12-26T07:19:02.104950Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nclass Flickr30kImagesDataset(Dataset):\n    # Initialize CLIP model and preprocessing transforms\n    # clip_model, _, clip_preprocess = create_model_and_transforms(\n    #     'ViT-bigG-14-quickgelu', pretrained='metaclip_fullcc', device='cuda'\n    # )\n    # clip_model.eval()  # Set to evaluation mode\n    \n    # def __init__(self, image_dir: str, captions_df: pd.DataFrame, prefix_length: int, normalize_prefix=False):\n    #     \"\"\"\n    #     image_dir: Path to the directory containing images.\n    #     captions_df: DataFrame with 'image' and 'caption' columns.\n    #     prefix_length: Length of the prefix for the embeddings.\n    #     normalize_prefix: Whether to normalize the CLIP embeddings.\n    #     \"\"\"\n    #     self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    #     self.prefix_length = prefix_length\n    #     self.normalize_prefix = normalize_prefix\n\n    #     # Precompute tokens and CLIP embeddings\n    #     self.caption_tokens = []\n    #     self.caption2embedding = []\n    #     self.max_seq_len = 0\n    #     self._precompute_embeddings_and_tokens(image_dir, captions_df)\n\n    # def _precompute_embeddings_and_tokens(self, image_dir, captions_df):\n    #     \"\"\"Precomputes tokens for captions and CLIP embeddings for images.\"\"\"\n    #     for idx, row in tqdm(captions_df.iterrows()):\n    #         image_filename = row['image']\n    #         caption = row['caption']\n\n    #         # Tokenize caption\n    #         tokens = torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64)\n    #         self.caption_tokens.append(tokens)\n    #         self.max_seq_len = max(self.max_seq_len, tokens.shape[0])\n\n    #         # Process image to get CLIP embedding\n    #         # image_path = os.path.join(image_dir, image_filename)\n    #         # image = Image.open(image_path)\n    #         # image_tensor = self.clip_preprocess(image).unsqueeze(0)  # Add batch dimension\n\n    #         # with torch.no_grad():\n    #         #     embedding = self.clip_model.encode_image(image_tensor).squeeze(0)\n    #         # if self.normalize_prefix:\n    #         #     embedding = embedding / embedding.norm(2, -1)\n    #         # self.caption2embedding.append(embedding)\n\n    #         if idx % 5 == 0:\n    #         # Process image to get CLIP embedding\n    #             image_path = os.path.join(image_dir, image_filename)\n    #             image = Image.open(image_path)\n    #             image_tensor = Flickr30kImagesDataset.clip_preprocess(image).unsqueeze(0).to('cuda')  # Add batch dimension\n    \n    #             with torch.no_grad():\n    #                 embedding = Flickr30kImagesDataset.clip_model.encode_image(image_tensor).squeeze(0)\n    #             if self.normalize_prefix:\n    #                 embedding = embedding / embedding.norm(2, -1)\n    #             self.caption2embedding.append(embedding)\n    #         else:\n    #             self.caption2embedding.append(self.caption2embedding[-1].clone())\n\n    def pad_tokens(self, idx):\n        \"\"\"Pads tokens to the maximum sequence length and creates a mask.\"\"\"\n        tokens = self.caption_tokens[idx]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.caption_tokens[idx] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            self.caption_tokens[idx] = tokens\n        mask = tokens.ge(0)  # Mask is zero where we are out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # Add prefix mask\n        return tokens, mask\n\n    def __len__(self):\n        return len(self.caption_tokens)\n\n    def __getitem__(self, idx):\n        tokens, mask = self.pad_tokens(idx)\n        prefix_embedding = self.caption2embedding[idx]\n        return tokens, mask, prefix_embedding\n\n    def to_pickle(self, pickle_path: str):\n        \"\"\"Saves the dataset information, including caption tokens, embeddings, and parameters, to a pickle file.\"\"\"\n        with open(pickle_path, 'wb') as f:\n            pickle.dump({\n                'caption_tokens': self.caption_tokens,\n                'caption2embedding': self.caption2embedding,\n                'max_seq_len': self.max_seq_len,\n                'prefix_length': self.prefix_length,\n                'normalize_prefix': self.normalize_prefix\n            }, f)\n\n    @classmethod\n    def from_pickle(cls, pickle_path: str):\n        \"\"\"Loads the dataset information, including caption tokens, embeddings, and parameters, from a pickle file.\"\"\"\n        with open(pickle_path, 'rb') as f:\n            data = pickle.load(f)\n        dataset = cls.__new__(cls)  # Create an uninitialized instance\n        dataset.caption_tokens = data['caption_tokens']\n        dataset.caption2embedding = data['caption2embedding']\n        dataset.max_seq_len = data['max_seq_len']\n        dataset.prefix_length = data['prefix_length']\n        dataset.normalize_prefix = data['normalize_prefix']\n        return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:19:02.106875Z","iopub.execute_input":"2024-12-26T07:19:02.107134Z","iopub.status.idle":"2024-12-26T07:19:02.128696Z","shell.execute_reply.started":"2024-12-26T07:19:02.107115Z","shell.execute_reply":"2024-12-26T07:19:02.127887Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\nclass MLP(nn.Module):\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n\n\nclass MlpTransformer(nn.Module):\n    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n        super().__init__()\n        out_d = out_d if out_d is not None else in_dim\n        self.fc1 = nn.Linear(in_dim, h_dim)\n        self.act = act\n        self.fc2 = nn.Linear(h_dim, out_d)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim_self // num_heads\n        self.scale = head_dim ** -0.5\n        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n        self.project = nn.Linear(dim_self, dim_self)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, y=None, mask=None):\n        y = y if y is not None else x\n        b, n, c = x.shape\n        _, m, d = y.shape\n        # b n h dh\n        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n        # b m 2 h dh\n        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n        if mask is not None:\n            if mask.dim() == 2:\n                mask = mask.unsqueeze(1)\n            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n        attention = attention.softmax(dim=2)\n        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n        out = self.project(out)\n        return out, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:19:02.130005Z","iopub.execute_input":"2024-12-26T07:19:02.130307Z","iopub.status.idle":"2024-12-26T07:19:02.149622Z","shell.execute_reply.started":"2024-12-26T07:19:02.130284Z","shell.execute_reply":"2024-12-26T07:19:02.148949Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nclass TransformerLayer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        x_, attention = self.attn(self.norm1(x), y, mask)\n        x = x + x_\n        x = x + self.mlp(self.norm2(x))\n        return x, attention\n\n    def forward(self, x, y=None, mask=None):\n        x = x + self.attn(self.norm1(x), y, mask)[0]\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n                 norm_layer: nn.Module = nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim_self)\n        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n        self.norm2 = norm_layer(dim_self)\n        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n\n\nclass Transformer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        attentions = []\n        for layer in self.layers:\n            x, att = layer.forward_with_attention(x, y, mask)\n            attentions.append(att)\n        return x, attentions\n\n    def forward(self, x, y=None, mask=None):\n        for i, layer in enumerate(self.layers):\n            if i % 2 == 0 and self.enc_dec: # cross\n                x = layer(x, y)\n            elif self.enc_dec:  # self\n                x = layer(x, x, mask)\n            else:  # self or cross\n                x = layer(x, y, mask)\n        return x\n\n    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n        super(Transformer, self).__init__()\n        dim_ref = dim_ref if dim_ref is not None else dim_self\n        self.enc_dec = enc_dec\n        if enc_dec:\n            num_layers = num_layers * 2\n        layers = []\n        for i in range(num_layers):\n            if i % 2 == 0 and enc_dec:  # cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            elif enc_dec:  # self\n                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            else:  # self or cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n        self.layers = nn.ModuleList(layers)\n\n\nclass TransformerMapper(nn.Module):\n\n    def forward(self, x):\n        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n        prefix = torch.cat((x, prefix), dim=1)\n        out = self.transformer(prefix)[:, self.clip_length:]\n        return out\n\n    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n        super(TransformerMapper, self).__init__()\n        self.clip_length = clip_length\n        self.transformer = Transformer(dim_embedding, 8, num_layers)\n        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:19:02.150496Z","iopub.execute_input":"2024-12-26T07:19:02.150766Z","iopub.status.idle":"2024-12-26T07:19:02.172457Z","shell.execute_reply.started":"2024-12-26T07:19:02.150745Z","shell.execute_reply":"2024-12-26T07:19:02.171658Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nclass ClipCaptionModel(nn.Module):\n\n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n\n    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n                labels: Optional[torch.Tensor] = None):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        if mapping_type == MappingType.MLP:\n            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n                                     self.gpt_embedding_size * prefix_length))\n        else:\n            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n                                                                     clip_length, num_layers)\n\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:19:02.173291Z","iopub.execute_input":"2024-12-26T07:19:02.173621Z","iopub.status.idle":"2024-12-26T07:19:02.190088Z","shell.execute_reply.started":"2024-12-26T07:19:02.173589Z","shell.execute_reply":"2024-12-26T07:19:02.189391Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\ndef save_config(args: argparse.Namespace):\n    config = {}\n    for key, item in args._get_kwargs():\n        config[key] = item\n    out_path = os.path.join(out_dir, f\"{args.prefix}.json\")\n    with open(out_path, 'w') as outfile:\n        json.dump(config, outfile)\n\n\ndef load_model(config_path: str, epoch_or_latest: Union[str, int] = '_latest'):\n    with open(config_path) as f:\n        config = json.load(f)\n    parser = argparse.ArgumentParser()\n    parser.set_defaults(**config)\n    # args = parser.parse_args()\n    if type(epoch_or_latest) is int:\n        epoch_or_latest = f\"-{epoch_or_latest:03d}\"\n    model_path = os.path.join(args.out_dir, f\"{args.prefix}{epoch_or_latest}.pt\")\n    if only_prefix:\n        model = ClipCaptionPrefix(prefix_length)\n    else:\n        model = ClipCaptionModel(prefix_length)\n    if os.path.isfile(model_path):\n        print(f\"loading model from {model_path}\")\n        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n    else:\n        print(f\"{model_path} is not exist\")\n    return model, parser\n\n\ndef train(dataset, model: ClipCaptionModel, batch_size, epochs , train_dataloader, val_dataloader,\n          lr: float = 2e-5 , \n          warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\", save_every = 1):\n    writer = SummaryWriter()\n    device = torch.device('cuda:0')\n    batch_size = batch_size\n    epochs = epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    model = model.to(device)\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=lr)\n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n    )\n    \n    for epoch in range(epochs):\n        print(f\">>> Training epoch {epoch}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        loss_train = None\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n            outputs = model(tokens, prefix, mask)\n            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n            loss_train = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n            loss_train.backward()\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            progress.set_postfix({\"loss_train\": loss_train.item()})\n            progress.update()\n            # break\n            # if (idx + 1) % 10000 == 0:\n            #     torch.save(\n            #         model.state_dict(),\n            #         os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n                # )\n        progress.close()\n        # writer.add_scalar(\"Loss/train\", loss_train, epoch)\n        # writer.flush()\n        # if epoch % save_every == 0 or epoch == epochs - 1:\n        torch.save(\n            model.state_dict(),\n            f\"{output_prefix}-epoch-{epoch:03d}-lossTrain-{loss_train:0.3f}.pt\" )\n\n\n        #evaluate\n        \n        model.eval()\n        loss_val = 0.0\n        with torch.no_grad():\n            for idx, (tokens, mask, prefix) in enumerate(val_dataloader):\n                tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n                outputs = model(tokens, prefix, mask)\n                logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n                loss_val += nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0).item()\n\n                # break\n        # Average validation loss\n        loss_val /= len(val_dataloader)\n        writer.add_scalar(\"Loss/train\", loss_train.item(), epoch)\n        writer.add_scalar(\"Loss/val\", loss_val, epoch)\n        writer.flush()\n        \n    writer.close()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:19:02.190840Z","iopub.execute_input":"2024-12-26T07:19:02.191073Z","iopub.status.idle":"2024-12-26T07:19:02.204274Z","shell.execute_reply.started":"2024-12-26T07:19:02.191054Z","shell.execute_reply":"2024-12-26T07:19:02.203443Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"SEED = 42\n\nout_dir = './checkpoints'\n# dataset = ClipCocoDataset(args.data, prefix_length, normalize_prefix=args.normalize_prefix)\ndata = '/kaggle/input/flickr-pickle-format/Flickr30kImagesDatasetSaveFinal'\ndataset = Flickr30kImagesDataset.from_pickle(data)\nprefix_length = 30\ndataset.prefix_length = prefix_length\n# prefix_dim = 640 if args.is_rn else 512\nprefix_dim = 1280","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:19:02.206322Z","iopub.execute_input":"2024-12-26T07:19:02.206578Z","iopub.status.idle":"2024-12-26T07:19:54.258071Z","shell.execute_reply.started":"2024-12-26T07:19:02.206559Z","shell.execute_reply":"2024-12-26T07:19:54.257384Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nbatch_size = 30\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:19:54.263466Z","iopub.execute_input":"2024-12-26T07:19:54.263702Z","iopub.status.idle":"2024-12-26T07:19:54.287513Z","shell.execute_reply.started":"2024-12-26T07:19:54.263684Z","shell.execute_reply":"2024-12-26T07:19:54.286648Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"mapping_type = {'mlp': MappingType.MLP, 'transformer': MappingType.Transformer}['transformer']\nonly_prefix = True\nif only_prefix:\n    model = ClipCaptionPrefix(prefix_length, clip_length=5, prefix_size=prefix_dim,\n                              num_layers=8, mapping_type=mapping_type)\n    print(\"Train only prefix\")\nelse:\n    model = ClipCaptionModel(prefix_length, clip_length=5, prefix_size=prefix_dim,\n                              num_layers=8, mapping_type=mapping_type)\n    print(\"Train both prefix and GPT\")\n    # sys.stdout.flush()\n\n\ntrain(dataset, model, batch_size = batch_size, train_dataloader = train_dataloader, val_dataloader = val_dataloader, epochs=10 , output_dir=\"./checkpoints\", output_prefix='Flickr30k')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:19:54.288382Z","iopub.execute_input":"2024-12-26T07:19:54.288669Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c3e230f05f94df39d0e6b3220fd52e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87514185c6d24cf1a733925239cea680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0aea1d58b36425abd5f6ee053eaef47"}},"metadata":{}},{"name":"stdout","text":"Train only prefix\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":">>> Training epoch 0\n","output_type":"stream"},{"name":"stderr","text":"Flickr30k: 100%|██████████| 4237/4237 [37:19<00:00,  1.89it/s, loss_train=2.7] \n","output_type":"stream"},{"name":"stdout","text":">>> Training epoch 1\n","output_type":"stream"},{"name":"stderr","text":"Flickr30k: 100%|██████████| 4237/4237 [36:48<00:00,  1.92it/s, loss_train=2.95]\n","output_type":"stream"},{"name":"stdout","text":">>> Training epoch 2\n","output_type":"stream"},{"name":"stderr","text":"Flickr30k: 100%|██████████| 4237/4237 [36:48<00:00,  1.92it/s, loss_train=2.49]\n","output_type":"stream"},{"name":"stdout","text":">>> Training epoch 3\n","output_type":"stream"},{"name":"stderr","text":"Flickr30k: 100%|██████████| 4237/4237 [36:48<00:00,  1.92it/s, loss_train=2.49]\n","output_type":"stream"},{"name":"stdout","text":">>> Training epoch 4\n","output_type":"stream"},{"name":"stderr","text":"Flickr30k: 100%|██████████| 4237/4237 [36:47<00:00,  1.92it/s, loss_train=2.38]\n","output_type":"stream"},{"name":"stdout","text":">>> Training epoch 5\n","output_type":"stream"},{"name":"stderr","text":"Flickr30k: 100%|██████████| 4237/4237 [36:47<00:00,  1.92it/s, loss_train=2.19]\n","output_type":"stream"},{"name":"stdout","text":">>> Training epoch 6\n","output_type":"stream"},{"name":"stderr","text":"Flickr30k: 100%|██████████| 4237/4237 [36:46<00:00,  1.92it/s, loss_train=2.28]\n","output_type":"stream"},{"name":"stdout","text":">>> Training epoch 7\n","output_type":"stream"},{"name":"stderr","text":"Flickr30k: 100%|██████████| 4237/4237 [36:45<00:00,  1.92it/s, loss_train=2.06]\n","output_type":"stream"},{"name":"stdout","text":">>> Training epoch 8\n","output_type":"stream"},{"name":"stderr","text":"Flickr30k:  92%|█████████▏| 3892/4237 [33:44<02:59,  1.92it/s, loss_train=2.14]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# %tensorboard --logdir logs","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\ntorch.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:58:31.885416Z","iopub.execute_input":"2024-12-24T20:58:31.885660Z","iopub.status.idle":"2024-12-24T20:58:35.098736Z","shell.execute_reply.started":"2024-12-24T20:58:31.885639Z","shell.execute_reply":"2024-12-24T20:58:35.097838Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'2.4.1+cu121'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# !pip install open-clip-torch --index-url https://pypi.org/simple/\n!pip install open_clip_torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:58:35.099649Z","iopub.execute_input":"2024-12-24T20:58:35.100109Z","iopub.status.idle":"2024-12-24T20:58:39.924403Z","shell.execute_reply.started":"2024-12-24T20:58:35.100075Z","shell.execute_reply":"2024-12-24T20:58:39.923319Z"}},"outputs":[{"name":"stdout","text":"Collecting open_clip_torch\n  Downloading open_clip_torch-2.29.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2.4.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.19.1+cu121)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2024.9.11)\nCollecting ftfy (from open_clip_torch)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (4.66.5)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.24.7)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.4.5)\nRequirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (1.0.12)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (2024.6.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (10.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\nDownloading open_clip_torch-2.29.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\nSuccessfully installed ftfy-6.3.1 open_clip_torch-2.29.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/'\nIMAGE_PATH = DATA_PATH + 'flickr30k_images'\ndf = pd.read_csv(DATA_PATH + 'captions.txt', delimiter=\",\")\ndf['comment'] = df['comment'].str.lstrip()\nprint(df.loc[19999, 'comment_number'], df.loc[19999, 'comment'], sep=' | ')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:58:39.925629Z","iopub.execute_input":"2024-12-24T20:58:39.925956Z","iopub.status.idle":"2024-12-24T20:58:40.326006Z","shell.execute_reply.started":"2024-12-24T20:58:39.925924Z","shell.execute_reply":"2024-12-24T20:58:40.325302Z"}},"outputs":[{"name":"stdout","text":"4 | A dog runs across the grass .\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df.columns = ['image', 'caption_number', 'caption']\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:58:40.328061Z","iopub.execute_input":"2024-12-24T20:58:40.328299Z","iopub.status.idle":"2024-12-24T20:58:40.341368Z","shell.execute_reply.started":"2024-12-24T20:58:40.328280Z","shell.execute_reply":"2024-12-24T20:58:40.340348Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                 image  caption_number  \\\n0       1000092795.jpg               0   \n1       1000092795.jpg               1   \n2       1000092795.jpg               2   \n3       1000092795.jpg               3   \n4       1000092795.jpg               4   \n...                ...             ...   \n158910   998845445.jpg               0   \n158911   998845445.jpg               1   \n158912   998845445.jpg               2   \n158913   998845445.jpg               3   \n158914   998845445.jpg               4   \n\n                                                  caption  \n0       Two young guys with shaggy hair look at their ...  \n1       Two young  White males are outside near many b...  \n2        Two men in green shirts are standing in a yard .  \n3            A man in a blue shirt standing in a garden .  \n4                 Two friends enjoy time spent together .  \n...                                                   ...  \n158910  A man in shorts and a Hawaiian shirt leans ove...  \n158911  A young man hanging over the side of a boat  w...  \n158912  A man is leaning off of the side of a blue and...  \n158913  A man riding a small boat in a harbor  with fo...  \n158914  A man on a moored blue and white boat with hil...  \n\n[158915 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption_number</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000092795.jpg</td>\n      <td>0</td>\n      <td>Two young guys with shaggy hair look at their ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000092795.jpg</td>\n      <td>1</td>\n      <td>Two young  White males are outside near many b...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000092795.jpg</td>\n      <td>2</td>\n      <td>Two men in green shirts are standing in a yard .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000092795.jpg</td>\n      <td>3</td>\n      <td>A man in a blue shirt standing in a garden .</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000092795.jpg</td>\n      <td>4</td>\n      <td>Two friends enjoy time spent together .</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>158910</th>\n      <td>998845445.jpg</td>\n      <td>0</td>\n      <td>A man in shorts and a Hawaiian shirt leans ove...</td>\n    </tr>\n    <tr>\n      <th>158911</th>\n      <td>998845445.jpg</td>\n      <td>1</td>\n      <td>A young man hanging over the side of a boat  w...</td>\n    </tr>\n    <tr>\n      <th>158912</th>\n      <td>998845445.jpg</td>\n      <td>2</td>\n      <td>A man is leaning off of the side of a blue and...</td>\n    </tr>\n    <tr>\n      <th>158913</th>\n      <td>998845445.jpg</td>\n      <td>3</td>\n      <td>A man riding a small boat in a harbor  with fo...</td>\n    </tr>\n    <tr>\n      <th>158914</th>\n      <td>998845445.jpg</td>\n      <td>4</td>\n      <td>A man on a moored blue and white boat with hil...</td>\n    </tr>\n  </tbody>\n</table>\n<p>158915 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from PIL import Image\nimport open_clip\nfrom torch.utils.data import Dataset\nfrom transformers import GPT2Tokenizer\nfrom open_clip.factory import create_model_and_transforms\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:58:40.343020Z","iopub.execute_input":"2024-12-24T20:58:40.343277Z","iopub.status.idle":"2024-12-24T20:58:45.074858Z","shell.execute_reply.started":"2024-12-24T20:58:40.343250Z","shell.execute_reply":"2024-12-24T20:58:45.073901Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Макс\n# df1 = df.iloc[0:80000]\n# df1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:58:45.075771Z","iopub.execute_input":"2024-12-24T20:58:45.076027Z","iopub.status.idle":"2024-12-24T20:58:45.079848Z","shell.execute_reply.started":"2024-12-24T20:58:45.076005Z","shell.execute_reply":"2024-12-24T20:58:45.078835Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Илюха\ndf2 = df.iloc[80000:]\ndf2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:58:45.080822Z","iopub.execute_input":"2024-12-24T20:58:45.081133Z","iopub.status.idle":"2024-12-24T20:58:45.108438Z","shell.execute_reply.started":"2024-12-24T20:58:45.081111Z","shell.execute_reply":"2024-12-24T20:58:45.107601Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                 image  caption_number  \\\n80000   3642088668.jpg               0   \n80001   3642088668.jpg               1   \n80002   3642088668.jpg               2   \n80003   3642088668.jpg               3   \n80004   3642088668.jpg               4   \n...                ...             ...   \n158910   998845445.jpg               0   \n158911   998845445.jpg               1   \n158912   998845445.jpg               2   \n158913   998845445.jpg               3   \n158914   998845445.jpg               4   \n\n                                                  caption  \n80000   A stewardess on an airplane pushes a cart down...  \n80001   A brunette flight attendant in a red uniform i...  \n80002   A flight attendant is pushing a beverage cart ...  \n80003   An airline stewardess is carefully rolling her...  \n80004   Flight attendant in red pushes drink cart thro...  \n...                                                   ...  \n158910  A man in shorts and a Hawaiian shirt leans ove...  \n158911  A young man hanging over the side of a boat  w...  \n158912  A man is leaning off of the side of a blue and...  \n158913  A man riding a small boat in a harbor  with fo...  \n158914  A man on a moored blue and white boat with hil...  \n\n[78915 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption_number</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>80000</th>\n      <td>3642088668.jpg</td>\n      <td>0</td>\n      <td>A stewardess on an airplane pushes a cart down...</td>\n    </tr>\n    <tr>\n      <th>80001</th>\n      <td>3642088668.jpg</td>\n      <td>1</td>\n      <td>A brunette flight attendant in a red uniform i...</td>\n    </tr>\n    <tr>\n      <th>80002</th>\n      <td>3642088668.jpg</td>\n      <td>2</td>\n      <td>A flight attendant is pushing a beverage cart ...</td>\n    </tr>\n    <tr>\n      <th>80003</th>\n      <td>3642088668.jpg</td>\n      <td>3</td>\n      <td>An airline stewardess is carefully rolling her...</td>\n    </tr>\n    <tr>\n      <th>80004</th>\n      <td>3642088668.jpg</td>\n      <td>4</td>\n      <td>Flight attendant in red pushes drink cart thro...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>158910</th>\n      <td>998845445.jpg</td>\n      <td>0</td>\n      <td>A man in shorts and a Hawaiian shirt leans ove...</td>\n    </tr>\n    <tr>\n      <th>158911</th>\n      <td>998845445.jpg</td>\n      <td>1</td>\n      <td>A young man hanging over the side of a boat  w...</td>\n    </tr>\n    <tr>\n      <th>158912</th>\n      <td>998845445.jpg</td>\n      <td>2</td>\n      <td>A man is leaning off of the side of a blue and...</td>\n    </tr>\n    <tr>\n      <th>158913</th>\n      <td>998845445.jpg</td>\n      <td>3</td>\n      <td>A man riding a small boat in a harbor  with fo...</td>\n    </tr>\n    <tr>\n      <th>158914</th>\n      <td>998845445.jpg</td>\n      <td>4</td>\n      <td>A man on a moored blue and white boat with hil...</td>\n    </tr>\n  </tbody>\n</table>\n<p>78915 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class Flickr30kImagesDataset(Dataset):\n    # Initialize CLIP model and preprocessing transforms\n    clip_model, _, clip_preprocess = create_model_and_transforms(\n        'ViT-bigG-14-quickgelu', pretrained='metaclip_fullcc', device='cuda'\n    )\n    clip_model.eval()  # Set to evaluation mode\n    \n    def __init__(self, image_dir: str, captions_df: pd.DataFrame, prefix_length: int, normalize_prefix=False):\n        \"\"\"\n        image_dir: Path to the directory containing images.\n        captions_df: DataFrame with 'image' and 'caption' columns.\n        prefix_length: Length of the prefix for the embeddings.\n        normalize_prefix: Whether to normalize the CLIP embeddings.\n        \"\"\"\n        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n\n        # Precompute tokens and CLIP embeddings\n        self.caption_tokens = []\n        self.caption2embedding = []\n        self.max_seq_len = 0\n        self._precompute_embeddings_and_tokens(image_dir, captions_df)\n\n    def _precompute_embeddings_and_tokens(self, image_dir, captions_df):\n        \"\"\"Precomputes tokens for captions and CLIP embeddings for images.\"\"\"\n        for idx, row in tqdm(captions_df.iterrows()):\n            image_filename = row['image']\n            caption = row['caption']\n\n            # Tokenize caption\n            tokens = torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64)\n            self.caption_tokens.append(tokens)\n            self.max_seq_len = max(self.max_seq_len, tokens.shape[0])\n\n            # Process image to get CLIP embedding\n            # image_path = os.path.join(image_dir, image_filename)\n            # image = Image.open(image_path)\n            # image_tensor = self.clip_preprocess(image).unsqueeze(0)  # Add batch dimension\n\n            # with torch.no_grad():\n            #     embedding = self.clip_model.encode_image(image_tensor).squeeze(0)\n            # if self.normalize_prefix:\n            #     embedding = embedding / embedding.norm(2, -1)\n            # self.caption2embedding.append(embedding)\n\n            if idx % 5 == 0:\n            # Process image to get CLIP embedding\n                image_path = os.path.join(image_dir, image_filename)\n                image = Image.open(image_path)\n                image_tensor = Flickr30kImagesDataset.clip_preprocess(image).unsqueeze(0).to('cuda')  # Add batch dimension\n    \n                with torch.no_grad():\n                    embedding = Flickr30kImagesDataset.clip_model.encode_image(image_tensor).squeeze(0)\n                if self.normalize_prefix:\n                    embedding = embedding / embedding.norm(2, -1)\n                self.caption2embedding.append(embedding)\n            else:\n                self.caption2embedding.append(self.caption2embedding[-1].clone())\n\n    def pad_tokens(self, idx):\n        \"\"\"Pads tokens to the maximum sequence length and creates a mask.\"\"\"\n        tokens = self.caption_tokens[idx]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.caption_tokens[idx] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            self.caption_tokens[idx] = tokens\n        mask = tokens.ge(0)  # Mask is zero where we are out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # Add prefix mask\n        return tokens, mask\n\n    def __len__(self):\n        return len(self.caption_tokens)\n\n    def __getitem__(self, idx):\n        tokens, mask = self.pad_tokens(idx)\n        prefix_embedding = self.caption2embedding[idx]\n        return tokens, mask, prefix_embedding\n\n    def to_pickle(self, pickle_path: str):\n        \"\"\"Saves the dataset information, including caption tokens, embeddings, and parameters, to a pickle file.\"\"\"\n        with open(pickle_path, 'wb') as f:\n            pickle.dump({\n                'caption_tokens': self.caption_tokens,\n                'caption2embedding': self.caption2embedding,\n                'max_seq_len': self.max_seq_len,\n                'prefix_length': self.prefix_length,\n                'normalize_prefix': self.normalize_prefix\n            }, f)\n\n    @classmethod\n    def from_pickle(cls, pickle_path: str):\n        \"\"\"Loads the dataset information, including caption tokens, embeddings, and parameters, from a pickle file.\"\"\"\n        with open(pickle_path, 'rb') as f:\n            data = pickle.load(f)\n        dataset = cls.__new__(cls)  # Create an uninitialized instance\n        dataset.caption_tokens = data['caption_tokens']\n        dataset.caption2embedding = data['caption2embedding']\n        dataset.max_seq_len = data['max_seq_len']\n        dataset.prefix_length = data['prefix_length']\n        dataset.normalize_prefix = data['normalize_prefix']\n        return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:58:45.109411Z","iopub.execute_input":"2024-12-24T20:58:45.109740Z","iopub.status.idle":"2024-12-24T21:03:19.341655Z","shell.execute_reply.started":"2024-12-24T20:58:45.109708Z","shell.execute_reply":"2024-12-24T21:03:19.340957Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/10.2G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090c5d3e1877477fbd4967a8b92d0dea"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def merge(first: Flickr30kImagesDataset, second: Flickr30kImagesDataset):\n    dataset = Flickr30kImagesDataset.__new__(Flickr30kImagesDataset)  # Create an uninitialized instance\n    dataset.caption_tokens = first.caption_tokens + second.caption_tokens\n    dataset.caption2embedding = first.caption2embedding + second.caption2embedding\n    dataset.max_seq_len = max(first.max_seq_len, second.max_seq_len)\n    dataset.prefix_length = first.prefix_length\n    dataset.normalize_prefix = first.normalize_prefix\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:03:19.342520Z","iopub.execute_input":"2024-12-24T21:03:19.342726Z","iopub.status.idle":"2024-12-24T21:03:19.347097Z","shell.execute_reply.started":"2024-12-24T21:03:19.342708Z","shell.execute_reply":"2024-12-24T21:03:19.346375Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Макс\n# dataset1 = Flickr30kImagesDataset(\n#     IMAGE_PATH,\n#     df1,\n#     5,\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:03:19.347800Z","iopub.execute_input":"2024-12-24T21:03:19.348083Z","iopub.status.idle":"2024-12-24T21:03:19.365212Z","shell.execute_reply.started":"2024-12-24T21:03:19.348063Z","shell.execute_reply":"2024-12-24T21:03:19.364593Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Илюха\ndataset2 = Flickr30kImagesDataset(\n    IMAGE_PATH,\n    df2,\n    5,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:03:19.366247Z","iopub.execute_input":"2024-12-24T21:03:19.366530Z","iopub.status.idle":"2024-12-24T21:54:48.957404Z","shell.execute_reply.started":"2024-12-24T21:03:19.366500Z","shell.execute_reply":"2024-12-24T21:54:48.956665Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f3b4b3bc3414888a755cacc9cde7637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f450d8f917c43d8952ba3f0a50b7aed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc42b6d3227648b09fa1fa9e104c5d5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec34047d2fa494193b8b34dc55fa8cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b9ca22dbe34e0b85209e7652978b76"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n78915it [51:28, 25.55it/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# dataset1.to_pickle(\"Flickr30kImagesDatasetSave1\") # Макс","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:54:48.958228Z","iopub.execute_input":"2024-12-24T21:54:48.958484Z","iopub.status.idle":"2024-12-24T21:54:48.961836Z","shell.execute_reply.started":"2024-12-24T21:54:48.958450Z","shell.execute_reply":"2024-12-24T21:54:48.961210Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset2.to_pickle(\"Flickr30kImagesDatasetSave2\") # Илюха","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:54:48.964208Z","iopub.execute_input":"2024-12-24T21:54:48.964435Z","iopub.status.idle":"2024-12-24T21:55:03.195629Z","shell.execute_reply.started":"2024-12-24T21:54:48.964415Z","shell.execute_reply":"2024-12-24T21:55:03.194929Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Теперь в Output есть файл, скачай его","metadata":{}},{"cell_type":"markdown","source":"# А дальше уже слияние, не запускаем пока","metadata":{}},{"cell_type":"code","source":"# check1 = Flickr30kImagesDataset.from_pickle(\"/kaggle/working/Flickr30kImagesDatasetSave1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:55:03.196591Z","iopub.execute_input":"2024-12-24T21:55:03.196805Z","iopub.status.idle":"2024-12-24T21:55:03.200077Z","shell.execute_reply.started":"2024-12-24T21:55:03.196787Z","shell.execute_reply":"2024-12-24T21:55:03.199337Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# check2 = Flickr30kImagesDataset.from_pickle(\"/kaggle/working/Flickr30kImagesDatasetSave2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:55:03.200812Z","iopub.execute_input":"2024-12-24T21:55:03.201028Z","iopub.status.idle":"2024-12-24T21:55:03.216244Z","shell.execute_reply.started":"2024-12-24T21:55:03.201010Z","shell.execute_reply":"2024-12-24T21:55:03.215466Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# final = merge(check1, check2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:55:03.217017Z","iopub.execute_input":"2024-12-24T21:55:03.217292Z","iopub.status.idle":"2024-12-24T21:55:03.233163Z","shell.execute_reply.started":"2024-12-24T21:55:03.217264Z","shell.execute_reply":"2024-12-24T21:55:03.232464Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# final.to_pickle(\"Flickr30kImagesDatasetSaveFinal\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:55:03.233865Z","iopub.execute_input":"2024-12-24T21:55:03.234127Z","iopub.status.idle":"2024-12-24T21:55:03.248984Z","shell.execute_reply.started":"2024-12-24T21:55:03.234108Z","shell.execute_reply":"2024-12-24T21:55:03.248238Z"}},"outputs":[],"execution_count":17}]}